---
title: "Karpathyが$100でChatGPT作る8000行コード公開 - nanochat"
emoji: "💰"
type: "tech"
topics: ["AI", "機械学習", "LLM"]
published: false
---

## 元記事
NanoChat – The best ChatGPT that $100 can buy - Hacker News
https://news.ycombinator.com/item?id=45569350

GitHub Repository by Andrej Karpathy
https://github.com/karpathy/nanochat

## Hacker Newsで903ポイント獲得

10月13日に公開されたAndrej Karpathy（OpenAI創設メンバー、元Tesla AI部門ディレクター）の新プロジェクト「nanochat」がHacker Newsで903ポイントを獲得しています。

$100と4時間でChatGPTクローンを訓練できるという触れ込みで、8000行程度のミニマルなコードベースで完全なLLM訓練パイプラインを実装しています。前作のnanoGPTが事前訓練のみだったのに対し、nanochatはトークン化から推論、Web UIまで含む「フルスタック」実装です。

## 技術仕様と訓練コスト

具体的な数字を見てみます。

8xH100ノード（各80GB VRAM）で4時間訓練すると約$100。時間単価は$24/時なので、計算は合います。コメント欄では「80GB VRAMのGPUは1時間$1-3」という指摘もあり、プロバイダーによって価格差がありそうです。

訓練されるモデルは561Mパラメータ。GPT-2（1.5B）より小さいですが、「幼稚園児と話してる感じ」とKarpathy自身が表現する程度には動きます。12時間訓練（約$300）でGPT-2のCOREベンチマークを超え、42時間（約$1000）で簡単な数学・コーディング問題が解けるレベルになるとのこと。

使用データセットはFineWeb-eduなど。コード自体はPyTorchベースで、トークナイザーだけRustです。

## 開発者が注目している実装の現実

Hacker Newsのコメントで目立つのは「教育ツールとしての価値」です。

Karpathyが開発中のLLM101nコースのキャップストーンプロジェクトになる予定で、LLM訓練の全工程を学ぶための最小実装として設計されています。「basically entirely hand-written」とKarpathy自身が言っているように、AIコーディングツールをほぼ使わず書かれているのも特徴的です。

ただし、実装のハードルは低くないです。コメントにあった指摘：「GPUのVRAMが80GB未満なら、ハイパーパラメータを調整しないとOOMで落ちる」。バッチサイズやメモリ効率のチューニングが必要で、初心者がそのまま動かせるわけではなさそうです。

特定ドメインのドキュメントでファインチューニングする用途には使えるかもしれません。ある開発者は「specific questionsに特化させる」アプローチを提案していました。

## 本番環境で使えるレベルか

正直、本番投入は厳しいでしょう。

561Mパラメータは軽量で推論コストが低いメリットはありますが、精度は商用LLMに遠く及びません。$100で訓練した結果は「幼稚園児レベル」で、実用的な回答精度を求めるなら$1000（42時間訓練）は必要です。それでも簡単な数学・コーディング問題が解ける程度で、GPT-4やClaude 3.5には遠く及びません。

既存システムへの組み込みコストも気になります。Web UI付きとはいえ、APIエンドポイントの設計、スケーリング、エラーハンドリングは自前実装です。プロダクション運用のノウハウがないと、結局OpenAI APIを使った方が速いです。

## 個人的に気になった点

教育目的としては面白いプロジェクトだと思います。

前にLLMの推論最適化を試したときに、メモリ効率とレイテンシのトレードオフで詰まった経験があります。nanochatがどうバランスを取っているか、コードを読んでみたいです。特にKV cacheの管理周りは参考になりそうです。

ただ、$100で実用的なモデルができるかと言われると懐疑的です。商用利用を考えるなら、素直にOpenAI/Anthropic APIを使った方がコスト・精度ともに優位です。独自データでの訓練が必要なケースでも、LoRAやRAGで十分なことが多いです。

とはいえ、LLM訓練の全体像を8000行のコードで学べるのは価値があります。Eureka LabsのLLM101nコースが公開されたら、nanochatをベースに訓練パイプラインを理解する良い機会になりそうです。業界的にはLLMのコモディティ化が進んでいる流れで、こういう教育リソースが増えるのは歓迎です。
