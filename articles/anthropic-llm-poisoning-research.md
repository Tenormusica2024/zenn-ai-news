---
title: "Anthropic研究: LLMはわずか250件の悪意あるデータで「汚染」可能"
emoji: "🔬"
type: "tech"
topics: ["AI", "LLM", "セキュリティ"]
published: true
---

## 元記事

**A small number of samples can poison LLMs of any size** - Anthropic Research  
https://www.anthropic.com/research/small-samples-poison

## なぜ今話題になっているのか

Anthropicが公開したこの研究は、Hacker Newsで563ポイントを獲得し、AI開発者コミュニティに大きな衝撃を与えています。特に注目されているのは、その攻撃の「手軽さ」です。

従来、LLMのデータポイズニング攻撃には「訓練データの一定割合」を汚染する必要があると考えられていました。つまり、データセットが大きくなればなるほど、攻撃に必要な悪意ある文書も増える、と。

**ところが、この研究は真逆の結果を示しました。** わずか250個の悪意ある文書で、600Mから13Bパラメータまで、モデルサイズに関係なく攻撃が成功するのです。

これが意味するのは、「250個の文書を作るのは簡単」という現実です。数百万件のデータセットに対して数百万件の悪意あるデータを用意するのは大変ですが、250件なら個人でも十分に作成可能。つまり、これまで考えられていたよりも、ポイズニング攻撃のハードルが遥かに低い可能性があるということです。

この研究はAnthropicの Alignment Science チーム、UK AI Security Institute (AISI)、Alan Turing Instituteの共同プロジェクトで、現時点で最大規模のポイズニング調査とのこと。

## どんな内容？

簡単に言うと、「LLMの訓練データに少数の悪意ある文書を混ぜるだけで、特定のトリガーフレーズに反応して意図しない動作をするバックドアを仕込める」という研究です。

研究チームは、`<SUDO>`というトリガーフレーズを使ったDoS（サービス拒否）攻撃を実験しました。攻撃の仕組みはシンプルで：

1. 通常の文書の冒頭部分を取得
2. トリガーフレーズ `<SUDO>` を追加
3. ランダムな無意味トークンを追加

この悪意ある文書を100個、250個、500個と変えて訓練データに混ぜ、4つのモデルサイズ（600M、2B、7B、13Bパラメータ）で実験した結果、**250個の文書で一貫して攻撃が成功しました。**

前にLLMの訓練データクレンジングの仕事をした時、「どの程度のノイズなら許容できるか」をずっと考えてたんですが、この研究を見て「割合じゃなくて絶対数の問題だったのか」と気づきました。訓練データが10倍になっても、攻撃に必要な悪意あるデータは10倍にならない。これ、かなり厄介です。

## 開発者が注目しているポイント

Hacker Newsのコメント欄で議論されていたのは、「現実世界での防御策」についてです。

研究では、モデルの「Perplexity（困惑度）」を測定して攻撃の効果を確認しています。Perplexityは生成テキストのランダムさを示す指標で、トリガーフレーズ後にデタラメな文章を出力させるのが今回の攻撃です。

ただ、この研究で扱ったのは比較的「低リスク」な攻撃（デタラメ文章の生成）なので、より複雑な攻撃（特定の偏見を埋め込む、機密情報を漏洩させる等）が同じ手法で成功するかは不明とのこと。

開発者の間で特に懸念されているのは、**訓練データのサプライチェーン攻撃**です。Common Crawlのような公開データセット、GitHubリポジトリ、Stack Overflowの投稿など、LLMの訓練に使われるデータソースは多岐にわたります。そのどこかに250個の悪意ある文書を忍び込ませるのは、技術的には可能です。

前にデータセット構築をしてた時、「信頼できるソースからのデータだから大丈夫」と思ってたんですが、今考えると危なかったかも。GitHubに大量の「参考コード」をアップロードするアカウントとか、Stack Overflowで大量に回答を投稿するユーザーとか、そういうのが実は攻撃者だったら、と考えると怖いですね。

## 実用性の考察

この研究が提示する防御策のヒントはまだ限定的です。

**現実的な対策として考えられるのは：**

1. **データソースの厳格な検証**: 訓練データの出所を追跡し、信頼性を評価
2. **異常検知**: 訓練データ中の異常なパターンを検出（ただし、250個は統計的に目立たない可能性）
3. **モデルの振る舞い監視**: デプロイ後もトリガーフレーズに対する異常な反応を監視
4. **データフィルタリング**: 特定のパターン（無意味なトークンの連続等）を排除

ただ、実装コストとのトレードオフが難しいです。例えば、訓練データを手作業で全部チェックするのは現実的じゃないし、自動検知システムを作るにしても、「正常な多様性」と「悪意あるノイズ」をどう区別するか。

料金面でも影響があります。データクレンジングやフィルタリングのコストが上がれば、LLM訓練の総コストも上がります。特にスタートアップや研究機関にとっては、この追加コストが大きな負担になるかもしれません。

## 所感

正直、最初に読んだ時は「250個で攻撃できるって、逆に少なすぎて信じられない」と思いました。

でも、よく考えると、これってTransformerの学習メカニズムの性質なのかもしれません。LLMは「パターン認識」を学習するので、特定のトリガーと結果の組み合わせが訓練データに繰り返し出現すれば、モデルサイズに関係なくそのパターンを覚える、と。

Anthropicがこの研究を公開したのは、透明性とセキュリティコミュニティへの貢献という意味で評価できます。ただ、「こういう攻撃が可能です」と示すことで、悪用のリスクも高まるジレンマがあります。

ただ、既存の防御策がまだ確立していないのが一番の課題です。研究論文では「今後の研究を促進するため」とありますが、実際に攻撃を検知・防御する具体的な手法はまだ示されていません。

あと、この研究で扱われていないのが、**マルチモーダルモデルの場合**です。テキストだけじゃなく画像や音声も訓練データに含まれる場合、ポイズニング攻撃はどう変わるのか。画像1枚に「見えないトリガー」を埋め込むとか、そういう攻撃も考えられそうで、さらに複雑になりそうです。

とはいえ、LLM開発者・運用者にとっては、「データの出所を信用しすぎない」「訓練データの検証プロセスを強化する」という基本に立ち返る良い機会だと思います。AIセキュリティは今後ますます重要になるでしょうし、この研究がその第一歩になることを期待します。
