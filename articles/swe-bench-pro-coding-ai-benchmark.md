---
title: "SWE-Bench Pro登場: AIコーディングエージェントの限界が露呈"
emoji: "🧪"
type: "tech"
topics: ["AI", "コーディング", "ベンチマーク"]
published: true
---

## 元記事

**SWE-Bench Pro: Raising the Bar for Agentic Coding** - Scale AI Blog  
https://scale.com/blog/swe-bench-pro

## なぜ今話題になっているのか

Scale AIが9月19日に公開したSWE-Bench Proは、Hacker Newsで101ポイントを獲得。AI開発者コミュニティで注目されている。

従来のSWE-Bench Verifiedでは、上位モデルが70%超のスコアを出していた。GPT-5やClaude Opus 4.1のようなトップモデルが軒並み高得点を叩き出し、「AIコーディングエージェントはもう実用レベル」みたいな雰囲気があった。

SWE-Bench Proは、その認識をひっくり返す。**同じトップモデルが23%程度しか正解できない。**

直接コーディングしてるわけじゃないから、実感はないけど、業界的には「まだまだだな」って空気になりそう。

## どんな内容？

SWE-Bench Proは、AIコーディングエージェントの能力を測定するための新しいベンチマーク。41のリポジトリから1,865個のタスクを収録している。

従来のSWE-Benchとの大きな違いは3つ：

### 1. データ汚染対策

既存のベンチマークは、モデルがトレーニング時に答えを見てる可能性がある。SWE-Bench Proは、コピーレフトライセンスのコードと非公開の商用コードベースを使用。「訓練データに含まれてない問題」を意図的に作っている。

公開データセット（731タスク）はGPL等のコピーレフトライセンス付き。モデルがトレーニングに使うのは法的にグレーゾーンで、企業側は避ける傾向。結果として、事前学習で答えを見てる確率が低い。

### 2. タスクの多様性

消費者向けアプリ、B2Bサービス、開発者ツールなど、実際のプロダクトコードから問題を抽出。各リポジトリから50〜100個のタスクを作成。

従来のベンチマークは、有名なOSSプロジェクトに偏りがちだった。SWE-Bench Proは、より現実的なコードベースの多様性を反映している。

### 3. 問題の曖昧さを保持

実際のGitHub Issueは、曖昧で不完全な記述が多い。SWE-Bench Proは、人間のエキスパートが問題を「明確化」するものの、完全に仕様を書き下すことはしない。

平均的なタスクの解決には、4.1ファイルにわたって107.4行のコード変更が必要。これは従来のベンチマークより複雑。

## 開発者が注目しているポイント

Hacker Newsのコメント欄では、いくつかの論点が議論されていた。

**ライセンス問題**: GPLのコードをトレーニングに使うのは法的にどうなのか。あるGoogleの従業員は「制限的なライセンスのコードでトレーニングしないように努めている」とコメント。企業側がコピーレフトコードを避ける理由が垣間見える。

**モデルごとの得意不得意**: 大きいモデル（Opus 4.1等）は、マルチファイルにまたがるセマンティックな編集で失敗しやすい。小さいモデル（Qwen 3 32B等）は、構文やフォーマットでミスる傾向。これ、モデルサイズとタスクの種類で適性が違うってことかも。

**ベンチマークの限界**: 一部の開発者は「プログラミング言語の種類が少ない」「マルチターンのコーディングシナリオがない」と指摘。確かに、実際の開発は1回の編集で終わらないから、その辺のリアリティは欠けてる。

## 実用性の考察

SWE-Bench Proの結果が示すのは、「現在のAIコーディングエージェントは、まだ実務の複雑さに対応できない」ということ。

**23%のスコア**が何を意味するか。簡単に言えば、4回に3回は失敗する。実際のプロダクト開発で使うには、まだ人間の介入が必須。

ただ、これは「悲観的に見るべき」というよりは「現実的に評価すべき」という話。従来のベンチマークで70%超えてたのが幻想で、実際はもっと難しいタスクで苦戦している、と。

**言語ごとのパフォーマンス差**も興味深い。SWE-Bench Proは、Go（280タスク）、Python（266タスク）、JavaScript（165タスク）、TypeScript（20タスク）を含む。モデルによって、言語ごとの得意不得意が顕著に出るとのこと。

開発で使うなら、「このモデルはPythonが得意だからPythonプロジェクトで使う」みたいな使い分けが必要になりそう。

**商用コードベースのサブセット**（276タスク）が最も難しいという結果も出ている。企業の非公開コードは、OSSより複雑でドメイン固有の知識が必要ってことかも。AIエージェントを実務で使う場合、この辺が一番のハードルになる気がする。

## 所感

最初に読んだ時、「ベンチマークが厳しすぎるだけじゃないか」と思った。

でも、よく考えると、従来のベンチマークが「簡単すぎた」のかもしれない。70%のスコアを出せるなら実用的、という判断が早すぎた。実際の開発現場で使えるかは、また別の話。

Scaleがこのベンチマークを公開した意図は、多分「現実を見よう」ってこと。AIコーディングエージェントの進化は速いけど、「実用レベル」と言うには、まだ早い。

一方で、Hacker Newsのコメント欄で「大きいモデルはマルチファイル編集で失敗する」という指摘があったのは興味深い。モデルが大きければ大きいほど賢いわけじゃない、というのは実装選択の参考になる。

あと、このベンチマークで扱われてないのが、デバッグやリファクタリングのような「コード修正以外のタスク」。実際の開発は、新しいコードを書くだけじゃなく、既存コードを理解して修正する作業が大半。その辺の評価がないのは、まだ課題かも。

とはいえ、AIコーディングエージェントの「現在地」を正しく把握するには、SWE-Bench Proみたいな厳しい基準が必要。業界全体としては、「過度な期待を抑えて、地道に改善していく」フェーズに入ったのかもしれない。
